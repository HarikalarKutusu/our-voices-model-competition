

# Our voices model competition

- What we are looking for ?
- Deadlines
- Rules

## About 

We want to see - and incentivise! - great diversity, equity and inclusion-conscious work being done with the Common Voice dataset.
We are running a model and methods competition with three broad themes, plus an open category.

### Register your intrest 

Our competition open on **September 15th** and runs till **October 12th**

Start today by [registering your interest with this form](https://mozillafoundation.typeform.com/to/TSTzyijc), and you'll receive a participant pack with guidance, resources, advice and more to help you. Please read the [full rules] if you are considering applying. 

## What are we looking for?

Your entry must be a diversity, equity and inclusion-conscious Model or Method under one of the following categories.
It must primarily make use of Mozilla Common Voice data from the 11th release (September 2022).
Outside of this, we are being deliberately open-ended. However, here are some illustrative examples;

| Categories             | About                                                                                     |
|--------------------|-------------------------------------------------------------------------------------------|
| Gender    | An STT model for an under-resourced language that performs equally well for women |
| Variant, Dialect or Accent | 1) Proof of concept for an under-served language variant delivered with a small ‘toy’ corpus 2) Accent classifiers by, and for, a community |
| Methods and Measures     |  1) A benchmark bias corpus 2) Dataset audit methodology                         |
|  Open         | Exciting DEI work primarily using Common Voice that doesn't fit into the categories above   |


---

## How are you making sure it's easy for all languages to participate?

- We are actively encouraging submissions at proof of concept stage that use a small or 'toy' corpus
- Our methodology and methods category enables teams to submit outlines for tools that they do not yet have the resources to build out further
- We have allowed a month of development time to accomodate those relying on CPU / slower compute
- Languages will be judged within 'Bands' - high resource, medium resource and low resource - to ensure a fairer competition between languages that exist in different contexts
- We are creating a flexible, holistic rubric that makes it possible for judges to look at ecosystem value-add factors beyond performance metrics like Word Error Rate
- For marginalised communities who have governance concerns about releasing their model under an open source license, they are welcome to submit with an explanation to that effect, and this will be considered accordingly

## Deadlines 

**July 6th** MCV 10 released

**July-September** Teams evaluate the data, then mobilise to grow and enhance datasets as needed for their ideas. There will also be changes to ask questions of the MCV team via an Ask Me Anything.

**September 14th** MCV 11 released

**September 15th** Competition opens

**October 12th** Competition closes

**Mid October (date TBC)** Judges meet, agree and Mozilla notifies winners

**Mid-Late October (date TBC** Winners announcement ceremony with demos at Speech Summit

## Our judges 

Who are the judging panel?

- Professor Francis Tyers - Computational Linguistics Advisor, Mozilla Foundation & Academic, University of Indiana Indiana
- Dr Vitaly Lavrukhin - Principal Applied Research Scientist, NVIDIA
- Wiebke Hutiri- PhD Candidate at Delft University of Technology - Fairness in Voice Tech
- Dr Abeba Birhane - AI Fellow Mozilla
- Rebecca Ryakitimbo - Community Fellow, Kiswahili
- Britone Mwasaru - Community Fellow, Kiswahili
- Dr Josh Meyers- Co-Founder, Coqui
- Stefania Delprete - Data Scientist and Italian MCV Community Rep
- Kathy Reid - PhD Candidate at Auatralia National University - Bias in Speech Tech, Open Source
- Gabriel Habayeb - Senior Data Engineer, Mozilla Foundation
